<details>
<summary>TAREA 2</summary>

# **Tarea 2**

---

El objetivo de esta tarea es realizar un an√°lisis tem√°tica del
libro: ‚ÄúA Critique of Democracy: a Guide for Neoreactionaries‚Äù( 2015) de M. Anissimov.
Para ello se usar√° Latent Dirichlet
Allocation (LDA). 
Si bien este m√©todo de an√°lisis tem√°tico ha [perdido
relevancia a ra√≠z de desarrollos en inteligencia artificial](https://towardsdatascience.com/is-lda-topic-modeling-dead-9543c18488fa/)
y de algunas limitaciones en su metodolog√≠a (la principal, que no estima el
n√∫mero de t√≥picos a evaluar sino que este n√∫mero debe ser indicado por el
investigador) igual se usar√° con el fin de evaluar su utilidad pr√°ctica en el
an√°lisis de textos relativamente peque√±os.
Para el an√°lisis se requieren las
siguientes librer√≠as:

<div style="font-size: small; line-height: 0.9;">
`library(tm)` #funciones de pre procesamiento de texto
<br>`library(epubr)` #abrir epub
<br>`library(dplyr)` #operaciones de pre procesamiento de texto
<br>`library(quanteda)` #crear dfm
<br>`library(topicmodels)` #analisis
<br>`library(readtext)` #abrir y crear archivo txt
<br>`library(textclean)` #transformar contracciones
<br>`library(SnowballC)` #para transformar palabas en sus raices (stemming)
<br>`library(tidyverse)` #graficos
</div>

---

## 1. **Ordenar**

El archivo que vamos a utilizar esta en extensi√≥n epub y se encuentra [aqu√≠](https://github.com/augusto-rp/tareas_curso/blob/master/tarea2_katebush/otros_textos/neoreaccionario.epub)

### Lo primero entonces es abrirlo

```r
epub_data <- epub("tarea2_katebush/otros_textos/neoreaccionario.epub")
#Extraemos el texto y generamos un solo vector

text_content <- epub_data$data[[1]]$text

full_text <- paste(text_content, collapse = "\n\n")
```

### Debemos hacer cierto procesamiento de datos 

```r
#poner todo en minuscula
full_text <- tolower(full_text)

#eliminar saltos de linea
gsub("\n", "", full_text)

#eliminar espacios donde haya multiples espacios
full_text <- gsub("\\s+", " ", full_text)

#Expandir contracciones en ingles
full_text <- replace_contraction(full_text)

#Eliminar puntuacion en full_text
full_text <- gsub("[[:punct:]]", " ", full_text)

#Eliminar stopwords de full_text. Estas son palabras que suelen de carecer contenido semantico en si mismo
full_text <- removeWords(full_text, stopwords("english"))
full_text
```
Ahora vamos a tokenizar el texto, es decir separarlo en "fichas" de palabras.
Esto permitira posteriormente acortar las palabras a su "raiz"

```r
#tokenizar full_text
full_text<- unlist(strsplit(full_text, " "))

#hacer stemming de full_text
full_text_stem <- wordStem(unlist(strsplit(full_text, " ")), language = "english")

#Para observar como queda el texto
full_text_stem
```

Y ahora crearemos un archivo txt con este outcome pues posteriormente convertiremos en coprus para poder realizar analisis

```r
#Archivo con stemming
writeLines(full_text_stem, "tarea2_katebush/otros_textos/output_file_stem.txt")
```


## 2. **Transformaci√≥n de datos**

El primer paso sera convertir el archivo txt en corpus pues este formateo es necesario para usar librerias de LDA

```r
#Convertirlo en corpus üëÅÔ∏è que estamos usando archivo creado en paso anterior üëÅÔ∏è
neor <- readLines("tarea2_katebush/otros_textos/output_file_stem.txt")
neo_c<- corpus(neor)

#Retokenizar archivo ahora en formato corpus
neo_tk<-tokens(neo_c)

#Esto lo convierte en un tipo de data frame (DFM) pero es solo un paso previo para convertirlo en el tipo de dataframe que usa topicmodels (DTM)
dfm_neo<-dfm(neo_tk)

#Y ahora si al formato DTM que es el que vamos a usar
dtm_neo <- convert(dfm_neo, to = "topicmodels")
```

Como veran hemos creado muchos objetos innecesarios en el camino, asi que vamos a borrarlos

```r
rm(list=c("dfm_neo","neo_tk","neo_c", "neor"))
```

Y ahora si podemos pasar a realizar los an√°lisis


## 3. **Modelar/Visualizar**
 Un primer paso es que tenemos que indicarle al codigo cuantos t√≥picos buscar, este es uno de las limitaciones de este tipo de an√°lisis que ha llevado a priorizar otro tipo de an√°lisis basados en IA.
 Pero no estoy en condiciones de aprender a programar en Phyton y generar un LLM en una semana
 
 Entonces primero haremos un modelo con 8 t√≥picos para ver como nos va
 
```r
 Siempre poner semilla!!!
set.seed(3141)
m_neo = LDA(dtm_neo,
            method = "Gibbs",
            k = 8,
            control = list(alpha = 0.5))  #ajuste el alpha a 0.5 que permite mayor solapamiento de palabras entre distintos topicos
            
#Solicitamos que nos diga las 8 palabras mas comunes
terms(m_neo, 8)
```

 Una observacion sobre **alpha** : entre m√°s alejado de 1 m√°s se supone que las "pertenence" a un solo t√≥pico. Y entre mas alejado de uno, mas se solapan entre si
 
 Vemos algunas cosas raras, como que en el topic 3 "s" es una palabra. Me entra la duda de si sera resultado de tokenizar el posesivo 
 
```r
set.seed(3141)
m_neo5 = LDA(dtm_neo,
            method = "Gibbs",
            k = 5,
            control = list(alpha = 0.5))
terms(m_neo5, 8)
```
 Aca ya empieza a ser un poco mas facil interpretar
 El topico 1 parece hablar de crecimiendo economico, el topico dos parece hablar de una mezcla de temas economicos y electorales, el 3 no queda tan claro
 El topico 4 menciona "hopp" que hace referencia a  a Hans-Hermann Hope un filosofo paleolibertario y anarcocapitalista. Por lo que podeos suponer que tiene que ver con argumentos de este autor
 El topico 5 parece hablar de aspectos sociales y culturales de la democracia
 
 Haremos un nuevo intento con 4 topicos
 
```r
set.seed(3141)
m_neo4 = LDA(dtm_neo,
             method = "Gibbs",
             k = 4,
             control = list(alpha = 0.5))
terms(m_neo4, 8)
```


## 4. **Comunicar**
¬†
![Se presentan 5 gr√°ficos de barras. Cada uno representa las
6 palabras con mayor relevancia para cada uno de los 5 topicos](https://github.com/augusto-rp/tareas_curso/blob/master/grafico_tarea2.jpeg)

</details>







<details>

  
<summary>TAREA 1</summary>

# **Tarea 1**

El objetivo de esta primera tarea es realizar un an√°lisis de las emociones presentes en un cap√≠tulo del programa "The Kardashians". Se utilizar√° la transcripci√≥n del d√©cimo cap√≠tulo de la primera temporada disponible en [el siguiente link](https://transcripts.foreverdreaming.org/viewforum.php?f=2354#google_vignette). Para un resumen del cap√≠tulo [pinchar aqu√≠](https://en.wikipedia.org/wiki/The_Kardashians#Season_1_(2022)).

Para ello se usar√° la librer√≠a tidytext que cuenta con distintos lexicones emocionales que permiten asignarles distintos valores a las palabras de acuerdo a diversos criterios:

‚Ä¢ Afinn: Asigna un valor entre -5 y +5 a las palabras de acuerdo a su valencia emocional. Siendo -5 extremadamente negativa y +5 extremadamente positiva

‚Ä¢ Bing: Binariza las palabras entre valencia positiva y negativa

‚Ä¢ NRC: Categoriza palabras en funci√≥n a categor√≠a emocional de pertenencia.

## **1. Ordenamiento de datos**

Al descargar la transcripci√≥n esta corresponde a un solo hilo de texto en formato txt. Hay que procesar el texto eliminando s√≠mbolos. Inicialmente se us√≥ la funci√≥n preprocess_text de la librer√≠a "text2emotion", sin embargo, esta modific√≥ el texto en algunas partes. Por ejemplo: "be able to expand`\r\n`our family one day" fue transformado a "be able to e tongue sticking out and our family one day"

Por lo tanto, se us√≥ una aproximaci√≥n distinta. Primero se eliminaron las apariciones del control `\r` en el archivo .txt que indica separaci√≥n entre l√≠neas, pero no cambio de turnos (Linea. 17). Posteriormente se separ√≥ el texto en l√≠neas individuales usando el control `\n` presente en el archivo txt como indicador para ello (L.26). Luego se eliminaron las l√≠neas vac√≠as (L.30,31). Finalmente se convirti√≥ la transcripci√≥n en un dataframe (L.66)


## **2. Transformaci√≥n de datos**

Se descargaron los lexicones se√±alados m√°s arriba y posteriormente se tokeniz√≥ el dataframe separ√°ndolo seg√∫n las palabras emocionales presentes en este (L69-81). Esto gener√≥ 3 objetos, cada uno incluyendo la categorizaci√≥n l√©xica de acuerdo a las caracter√≠sticas de cada uno de los lexicones. 

En base a estos objetos se puede empezar a generar gr√°ficos para visualizar los resultados.

## **3. Visualizacion**
Se intentaron diversas aproximaciones para graficar los resultados, las cu√°les ayudaron a su vez a visibilizar las limitaciones de este tipo de an√°lisis (ver secci√≥n siguiente).
Primero se hizo una tabla con las 3 palabras m√°s comunes por cada emoci√≥n categorizada por el lexic√≥n nrc, y posteriormente esta se grafic√≥. Sin embargo, este gr√°fico fue considerado inapropiado ya que una palabra puede ser agrupada en m√°s de una categor√≠a en una aparici√≥n lo que distorsiona entonces los resultados. 

Se consider√≥ m√°s apropiado realizar un gr√°fico de barras para mostrar la frecuencia de las 4 palabras m√°s comunes para expresar sentimientos positivos y negativos, que se presenta a continuaci√≥n.

![Grafico de barras que muestra las 4 palabras m√°s usadas para demostrar sentimientos positivos y negativos. En el eje Y se indica la frecuencia de aparici√≥n que va desde 0 hasta 129. Las palabras m√°s usadas para demostrar sentimientos positivos son ‚Äúlike‚Äù, ‚ÄúGood‚Äù, ‚Äúlove‚Äù y ‚Äúright‚Äù. Las 4 m√°s usadas para sentimientos negativos son ‚Äúbad‚Äù, ‚Äúhard‚Äù, ‚Äúcrazy‚Äù y‚Äùexhausted‚Äù ](https://github.com/augusto-rp/tareas_curso/blob/master/tarea1_kardashians/grafico_bin.jpeg) ‚ÄúPalabras m√°s comunes para expresar emociones positivas y negativas‚Äù



## **4. Conclusiones**
Los an√°lisis demuestran las limitaciones de este paquete para realizar el an√°lisis de las emociones presentes en el cap√≠tulo. La principal limitaci√≥n es la ausencia de consideraciones contextuales y pragm√°ticas en la categorizaci√≥n de las palabras. Esto es evidente en la asignaci√≥n de valencias positivas a las palabra ‚Äúlike‚Äù que en la mayor√≠a de los casos es usada no para indicar gusto por algo sino como muletilla al hablar.

Adem√°s, en el lexic√≥n NRC una misma palabra puede corresponden a m√∫ltiples emociones, por ejemplo ‚Äúgod‚Äù es considerada como indicando anticipaci√≥n, jubilo, miedo, confianza y sentimientos positivos. Sin embargo, al ver en detalle la aparici√≥n de esta palabra en la mayor√≠a de los casos se trata de la expresi√≥n ‚Äúoh my god‚Äù.

Un aprendizaje de esta tarea es entonces, que en caso de usar estos lexicones es importante no depender exclusivamente de ellos para el an√°lisis sem√°nticos de las emociones presentes y la importancia de estar familiarizado con el texto.

Finalmente, es importante se√±alar que debido a las caracteristicas del archivo de transcripcion original no se logr√≥ separar los turnos de habla por hablante. Lo que revela tambi√©n la relevancia de contar con fuentes de datos apropiadamente formateadas para facilitiar sus an√°lisis.

</details>
